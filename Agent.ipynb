{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c226bdf7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:12.633281Z",
     "iopub.status.busy": "2025-04-20T16:53:12.632766Z",
     "iopub.status.idle": "2025-04-20T16:53:13.813954Z",
     "shell.execute_reply": "2025-04-20T16:53:13.812627Z"
    },
    "papermill": {
     "duration": 1.190887,
     "end_time": "2025-04-20T16:53:13.815836",
     "exception": false,
     "start_time": "2025-04-20T16:53:12.624949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with ma\n",
    "# ny helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a6c954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:13.829446Z",
     "iopub.status.busy": "2025-04-20T16:53:13.828902Z",
     "iopub.status.idle": "2025-04-20T16:53:23.621891Z",
     "shell.execute_reply": "2025-04-20T16:53:23.619890Z"
    },
    "papermill": {
     "duration": 9.802454,
     "end_time": "2025-04-20T16:53:23.624312",
     "exception": false,
     "start_time": "2025-04-20T16:53:13.821858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.5)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.6)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.12.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries for this notebook\n",
    "!pip install -U google-generativeai python-dotenv pymupdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e26116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:13:02.856163Z",
     "iopub.status.busy": "2025-04-09T13:13:02.855914Z",
     "iopub.status.idle": "2025-04-09T13:13:02.863109Z",
     "shell.execute_reply": "2025-04-09T13:13:02.861760Z",
     "shell.execute_reply.started": "2025-04-09T13:13:02.856138Z"
    },
    "papermill": {
     "duration": 0.005492,
     "end_time": "2025-04-20T16:53:23.636013",
     "exception": false,
     "start_time": "2025-04-20T16:53:23.630521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TaskFlow: An Autonomous Research & Execution Agent üß†‚öôÔ∏è\n",
    "\n",
    "This project demonstrates a GenAI-based autonomous agent that:\n",
    "- Understands complex research queries\n",
    "- Breaks them into subtasks\n",
    "- Retrieves and summarizes relevant info using LLM\n",
    "- Outputs structured research reports (in JSON/Markdown)\n",
    "\n",
    "Models used: `gemini-1.5-pro`, `text-embedding-004`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d139c1d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:23.649147Z",
     "iopub.status.busy": "2025-04-20T16:53:23.648770Z",
     "iopub.status.idle": "2025-04-20T16:53:25.089659Z",
     "shell.execute_reply": "2025-04-20T16:53:25.088514Z"
    },
    "papermill": {
     "duration": 1.450096,
     "end_time": "2025-04-20T16:53:25.091837",
     "exception": false,
     "start_time": "2025-04-20T16:53:23.641741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "\n",
    "# Show the installed google-generativeai version\n",
    "genai.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0661476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "for m in genai.list_models():\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4531ff0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:25.105700Z",
     "iopub.status.busy": "2025-04-20T16:53:25.105013Z",
     "iopub.status.idle": "2025-04-20T16:53:25.599947Z",
     "shell.execute_reply": "2025-04-20T16:53:25.598606Z"
    },
    "papermill": {
     "duration": 0.504105,
     "end_time": "2025-04-20T16:53:25.602168",
     "exception": false,
     "start_time": "2025-04-20T16:53:25.098063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm ready. How can I assist you with this test?\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Hello! This is a test.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f332a15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:25.616300Z",
     "iopub.status.busy": "2025-04-20T16:53:25.615876Z",
     "iopub.status.idle": "2025-04-20T16:53:25.882482Z",
     "shell.execute_reply": "2025-04-20T16:53:25.881186Z"
    },
    "papermill": {
     "duration": 0.275907,
     "end_time": "2025-04-20T16:53:25.884661",
     "exception": false,
     "start_time": "2025-04-20T16:53:25.608754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "def is_retriable(exception):\n",
    "    return isinstance(exception, Exception) and (\n",
    "        \"429\" in str(exception) or \"503\" in str(exception)\n",
    "    )\n",
    "\n",
    "retry_wrapper = retry.Retry(predicate=is_retriable)\n",
    "\n",
    "def generate_with_retry(model, *args, **kwargs):\n",
    "    return retry_wrapper(model.generate_content)(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e305d73",
   "metadata": {
    "papermill": {
     "duration": 0.005661,
     "end_time": "2025-04-20T16:53:26.005087",
     "exception": false,
     "start_time": "2025-04-20T16:53:25.999426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Using the gemini-1.5-pro, and answering a prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e2645b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:26.018259Z",
     "iopub.status.busy": "2025-04-20T16:53:26.017837Z",
     "iopub.status.idle": "2025-04-20T16:53:30.764381Z",
     "shell.execute_reply": "2025-04-20T16:53:30.762886Z"
    },
    "papermill": {
     "duration": 4.755484,
     "end_time": "2025-04-20T16:53:30.766532",
     "exception": false,
     "start_time": "2025-04-20T16:53:26.011048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of some of the best open-source LLMs with their license information:\n",
      "\n",
      "1.  **Meta Llama 2 & 3:**\n",
      "    *   **Strengths:** Leading in performance and adoption, available in various sizes (7B, 13B, 70B, 8B, 70B, 400B+). Excellent for a wide range of tasks.\n",
      "    *   **License:** Meta Llama Community Licenses (Llama 2 Community License, Meta Llama 3 Community License). These generally permit commercial use, but include revenue-based restrictions for very large enterprises (over $700M USD annual revenue) requiring special licensing.\n",
      "\n",
      "2.  **Mistral AI Models (Mistral 7B, Mixtral 8x7B):**\n",
      "    *   **Strengths:** Exceptional performance-to-size ratio. Mixtral 8x7B is a Mixture-of-Experts (MoE) model offering high quality with efficient inference.\n",
      "    *   **License:** Apache 2.0. This is a highly permissive open-source license, allowing extensive commercial use, modification, and distribution with attribution.\n",
      "\n",
      "3.  **Google Gemma:**\n",
      "    *   **Strengths:** Lightweight, open models (2B, 7B) designed with responsible AI principles. Good for research and constrained environments.\n",
      "    *   **License:** Gemma License. It permits commercial use but includes specific terms and conditions, focusing on responsible development and use.\n",
      "\n",
      "4.  **Microsoft Phi-3:**\n",
      "    *   **Strengths:** A family of small yet powerful models (Mini, Small, Medium). Ideal for on-device inference, edge applications, and scenarios with limited resources where performance is still crucial.\n",
      "    *   **License:** MIT License. This is an extremely permissive license, allowing commercial use, modification, distribution, and private use with minimal restrictions, primarily requiring copyright and permission notices.\n",
      "\n",
      "These models offer diverse capabilities and licensing terms, making them suitable for a wide array of projects and development needs.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "short_config = {\"max_output_tokens\": 2000}\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"Find best open-source LLMs with license info in 200 words\",\n",
    "    generation_config=short_config,\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c460d34",
   "metadata": {
    "papermill": {
     "duration": 0.006044,
     "end_time": "2025-04-20T16:53:30.778824",
     "exception": false,
     "start_time": "2025-04-20T16:53:30.772780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Responding some user queries and returning the response in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e3bb5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:30.792754Z",
     "iopub.status.busy": "2025-04-20T16:53:30.792377Z",
     "iopub.status.idle": "2025-04-20T16:53:36.184868Z",
     "shell.execute_reply": "2025-04-20T16:53:36.183268Z"
    },
    "papermill": {
     "duration": 5.403069,
     "end_time": "2025-04-20T16:53:36.188120",
     "exception": false,
     "start_time": "2025-04-20T16:53:30.785051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tasks\": [\n",
      "    \"Identify current applications and case studies of AI in various sustainable development sectors (e.g., renewable energy, smart agriculture, climate modeling, waste management).\",\n",
      "    \"Analyze the potential benefits and contributions of AI technologies towards achieving specific United Nations Sustainable Development Goals (SDGs).\",\n",
      "    \"Examine the challenges, risks, and ethical considerations associated with deploying AI for sustainable development, including energy consumption, data privacy, bias, and job displacement.\",\n",
      "    \"Investigate policy frameworks, governance models, and international collaborations required to ensure responsible and equitable AI development and deployment for sustainability.\",\n",
      "    \"Explore future trends and emerging AI technologies that could further accelerate progress in sustainable development and address global environmental and social challenges.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import typing_extensions as typing\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "class TaskPlan(typing.TypedDict):\n",
    "    tasks: list[str]\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "    \"response_schema\": TaskPlan,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "def plan_tasks(topic: str):\n",
    "    response = model.generate_content(\n",
    "        f\"Break into 3‚Äì5 research tasks about: '{topic}'. \"\n",
    "        \"Return JSON with a single key 'tasks' containing an array of task strings.\",\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "user_query = \"Role of AI in sustainable development\"  # queries\n",
    "tasks = plan_tasks(user_query)\n",
    "\n",
    "parsed_tasks = json.loads(tasks)\n",
    "print(json.dumps(parsed_tasks, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac487338",
   "metadata": {
    "papermill": {
     "duration": 0.007287,
     "end_time": "2025-04-20T16:53:36.202929",
     "exception": false,
     "start_time": "2025-04-20T16:53:36.195642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Agent Loop\n",
    "[ User query ] ‚Üí [ Task planner ] ‚Üí [ Subtasks ] ‚Üí [ Execute each task ]\n",
    "\n",
    "- It decomposes each tasks into subtasks\n",
    "- Then executes each subtasks using GenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db713ad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:36.216948Z",
     "iopub.status.busy": "2025-04-20T16:53:36.216558Z",
     "iopub.status.idle": "2025-04-20T16:53:42.054376Z",
     "shell.execute_reply": "2025-04-20T16:53:42.052761Z"
    },
    "papermill": {
     "duration": 5.847163,
     "end_time": "2025-04-20T16:53:42.056438",
     "exception": false,
     "start_time": "2025-04-20T16:53:36.209275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Task 1: Identify current applications and case studies of AI in various sustainable development sectors (e.g., renewable energy, smart agriculture, climate modeling, waste management).\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\nPlease retry in 44.210378852s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 5\n}\n, retry_delay {\n  seconds: 44\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîπ Task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Ask Gemini to answer each task\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a short paragraph on: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìù Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\nPlease retry in 44.210378852s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 5\n}\n, retry_delay {\n  seconds: 44\n}\n]"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.5-flash\") # using a different model as gemini 1.5 pro has limitations on rpm\n",
    "\n",
    "\n",
    "\n",
    "for i, task in enumerate(parsed_tasks['tasks'], 1):\n",
    "    print(f\"\\nüîπ Task {i}: {task}\")\n",
    "    \n",
    "    # Ask Gemini to answer each task\n",
    "    response = model.generate_content(f\"Write a short paragraph on: {task}\")\n",
    "    \n",
    "    print(\"üìù Response:\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e7fe1",
   "metadata": {
    "papermill": {
     "duration": 0.006485,
     "end_time": "2025-04-20T16:53:42.070069",
     "exception": false,
     "start_time": "2025-04-20T16:53:42.063584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Document Understanding\n",
    "\n",
    "Reading documents and extracting text from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad31437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:42.086986Z",
     "iopub.status.busy": "2025-04-20T16:53:42.086481Z",
     "iopub.status.idle": "2025-04-20T16:53:49.745776Z",
     "shell.execute_reply": "2025-04-20T16:53:49.744428Z"
    },
    "papermill": {
     "duration": 7.671201,
     "end_time": "2025-04-20T16:53:49.747934",
     "exception": false,
     "start_time": "2025-04-20T16:53:42.076733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_22608\\1839195758.py:12: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  raw_text = extract_text_from_pdf('H:\\GATE SYLLABUS.pdf')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install pymupdf\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "raw_text = extract_text_from_pdf('H:\\GATE SYLLABUS.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb36799",
   "metadata": {
    "papermill": {
     "duration": 0.008487,
     "end_time": "2025-04-20T16:53:49.765152",
     "exception": false,
     "start_time": "2025-04-20T16:53:49.756665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Breaking the long pdf into chunks so that it is made easier for the model to understand and give output efficiently with minimal errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2bfab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:49.782139Z",
     "iopub.status.busy": "2025-04-20T16:53:49.781771Z",
     "iopub.status.idle": "2025-04-20T16:53:49.790303Z",
     "shell.execute_reply": "2025-04-20T16:53:49.788686Z"
    },
    "papermill": {
     "duration": 0.019776,
     "end_time": "2025-04-20T16:53:49.792437",
     "exception": false,
     "start_time": "2025-04-20T16:53:49.772661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=100, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76e909",
   "metadata": {
    "papermill": {
     "duration": 0.006967,
     "end_time": "2025-04-20T16:53:49.807647",
     "exception": false,
     "start_time": "2025-04-20T16:53:49.800680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Embed the chunks\n",
    "\n",
    "Embedding the chunks into vectors for semantic understanding.\n",
    "\n",
    "using embedding model for embedding the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5206b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:49.823690Z",
     "iopub.status.busy": "2025-04-20T16:53:49.823266Z",
     "iopub.status.idle": "2025-04-20T16:54:12.838397Z",
     "shell.execute_reply": "2025-04-20T16:54:12.837066Z"
    },
    "papermill": {
     "duration": 23.025452,
     "end_time": "2025-04-20T16:54:12.840309",
     "exception": false,
     "start_time": "2025-04-20T16:53:49.814857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batching the chunks as the request is limited to 100\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "\n",
    "def batch_chunks(chunks, batch_size=100):\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        yield chunks[i:i + batch_size]\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for batch in batch_chunks(chunks, batch_size=100):\n",
    "    response = genai.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        content=batch,\n",
    "        task_type=\"semantic_similarity\",\n",
    "    )\n",
    "\n",
    "    # Case 1: batch of many ‚Üí response[\"embeddings\"]\n",
    "    if \"embeddings\" in response:\n",
    "        vectors = [item[\"embedding\"] for item in response[\"embeddings\"]]\n",
    "\n",
    "    # Case 2: batch of one ‚Üí response[\"embedding\"]\n",
    "    elif \"embedding\" in response:\n",
    "        vectors = [response[\"embedding\"]]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected embedding response format:\", response)\n",
    "\n",
    "    # FIX: Convert token-level embeddings (N,768) to sentence-level (768,)\n",
    "    processed_vectors = []\n",
    "    for v in vectors:\n",
    "        v = np.array(v)\n",
    "        if v.ndim == 2:            # shape (N, 768)\n",
    "            v = v.mean(axis=0)     # convert to (768,)\n",
    "        processed_vectors.append(v.tolist())\n",
    "\n",
    "    all_embeddings.extend(processed_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea91648",
   "metadata": {
    "papermill": {
     "duration": 0.007897,
     "end_time": "2025-04-20T16:54:12.855902",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.848005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The embeddedd vectors are now being stored in chunk_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648020f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:12.871914Z",
     "iopub.status.busy": "2025-04-20T16:54:12.871560Z",
     "iopub.status.idle": "2025-04-20T16:54:12.888671Z",
     "shell.execute_reply": "2025-04-20T16:54:12.887262Z"
    },
    "papermill": {
     "duration": 0.027561,
     "end_time": "2025-04-20T16:54:12.890922",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.863361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming all_embeddings and chunks are in order\n",
    "chunk_db = [\n",
    "    {\"text\": chunk, \"embedding\": embedding}\n",
    "    for chunk, embedding in zip(chunks, all_embeddings)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca2831",
   "metadata": {
    "papermill": {
     "duration": 0.008326,
     "end_time": "2025-04-20T16:54:12.907874",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.899548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Also embedding the query into vectors so that the model can match these embedding with the relevant content from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78860ca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:12.924378Z",
     "iopub.status.busy": "2025-04-20T16:54:12.923962Z",
     "iopub.status.idle": "2025-04-20T16:54:12.928907Z",
     "shell.execute_reply": "2025-04-20T16:54:12.927409Z"
    },
    "papermill": {
     "duration": 0.01543,
     "end_time": "2025-04-20T16:54:12.930839",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.915409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"what are the important topics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b62bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:12.947075Z",
     "iopub.status.busy": "2025-04-20T16:54:12.946742Z",
     "iopub.status.idle": "2025-04-20T16:54:13.127730Z",
     "shell.execute_reply": "2025-04-20T16:54:13.126568Z"
    },
    "papermill": {
     "duration": 0.19152,
     "end_time": "2025-04-20T16:54:13.130032",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.938512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "query_embedding_response = genai.embed_content(\n",
    "    model=\"text-embedding-004\",\n",
    "    content=query,\n",
    "    task_type=\"semantic_similarity\",\n",
    ")\n",
    "\n",
    "# Single embedding vector for the query\n",
    "query_embedding = query_embedding_response.get(\"embedding\") or query_embedding_response.get(\"embeddings\", [None])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe62053",
   "metadata": {
    "papermill": {
     "duration": 0.006919,
     "end_time": "2025-04-20T16:54:13.144249",
     "exception": false,
     "start_time": "2025-04-20T16:54:13.137330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Calculating the similarity scores using cosine similarity and finding the top k similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092ac5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:13.160454Z",
     "iopub.status.busy": "2025-04-20T16:54:13.160018Z",
     "iopub.status.idle": "2025-04-20T16:54:13.536617Z",
     "shell.execute_reply": "2025-04-20T16:54:13.535385Z"
    },
    "papermill": {
     "duration": 0.388596,
     "end_time": "2025-04-20T16:54:13.540117",
     "exception": false,
     "start_time": "2025-04-20T16:54:13.151521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "scored_chunks = [\n",
    "    (cosine_similarity(np.array(query_embedding), np.array(chunk[\"embedding\"])), chunk[\"text\"])\n",
    "    for chunk in chunk_db\n",
    "]\n",
    "\n",
    "ranked_chunks = sorted(scored_chunks, key=lambda x: x[0], reverse=True)\n",
    "top_chunks = [text for _, text in ranked_chunks[:top_k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a5a59",
   "metadata": {
    "papermill": {
     "duration": 0.007553,
     "end_time": "2025-04-20T16:54:13.557185",
     "exception": false,
     "start_time": "2025-04-20T16:54:13.549632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Giving a rag prompt that specifies what to do to the model\n",
    "\n",
    "and generating the output using the gemini model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      " \n",
      "CS Computer Science and Information Technology \n",
      "Section 1: Engineering Mathematics \n",
      "Discrete Mathe\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks[0]))\n",
    "print(chunks[0][:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d43e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:13.575184Z",
     "iopub.status.busy": "2025-04-20T16:54:13.574705Z",
     "iopub.status.idle": "2025-04-20T16:54:19.655672Z",
     "shell.execute_reply": "2025-04-20T16:54:19.653583Z"
    },
    "papermill": {
     "duration": 6.092854,
     "end_time": "2025-04-20T16:54:19.657782",
     "exception": false,
     "start_time": "2025-04-20T16:54:13.564928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final Answer:\n",
      " Based on the provided context, the important topic mentioned is **Discrete Mathematics**.\n",
      "\n",
      "The context does not provide a further breakdown of specific important topics *within* Discrete Mathematics.\n"
     ]
    }
   ],
   "source": [
    "rag_prompt = (\n",
    "    \"Answer the following question using the provided context.\\n\\n\"\n",
    "    f\"Context:\\n{''.join(top_chunks)}\\n\\n\"\n",
    "    f\"Question:\\n{query}\"\n",
    ")\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "response = genai.GenerativeModel(\"gemini-2.5-flash\").generate_content(rag_prompt)\n",
    "\n",
    "print(\"üîç Final Answer:\\n\", response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7109811,
     "sourceId": 11359831,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.316495,
   "end_time": "2025-04-20T16:54:22.914277",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T16:53:08.597782",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
