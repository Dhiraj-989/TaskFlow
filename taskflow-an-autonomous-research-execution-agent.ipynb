{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c226bdf7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:12.633281Z",
     "iopub.status.busy": "2025-04-20T16:53:12.632766Z",
     "iopub.status.idle": "2025-04-20T16:53:13.813954Z",
     "shell.execute_reply": "2025-04-20T16:53:13.812627Z"
    },
    "papermill": {
     "duration": 1.190887,
     "end_time": "2025-04-20T16:53:13.815836",
     "exception": false,
     "start_time": "2025-04-20T16:53:12.624949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with ma\n",
    "# ny helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a6c954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:13.829446Z",
     "iopub.status.busy": "2025-04-20T16:53:13.828902Z",
     "iopub.status.idle": "2025-04-20T16:53:23.621891Z",
     "shell.execute_reply": "2025-04-20T16:53:23.619890Z"
    },
    "papermill": {
     "duration": 9.802454,
     "end_time": "2025-04-20T16:53:23.624312",
     "exception": false,
     "start_time": "2025-04-20T16:53:13.821858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e26116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:13:02.856163Z",
     "iopub.status.busy": "2025-04-09T13:13:02.855914Z",
     "iopub.status.idle": "2025-04-09T13:13:02.863109Z",
     "shell.execute_reply": "2025-04-09T13:13:02.861760Z",
     "shell.execute_reply.started": "2025-04-09T13:13:02.856138Z"
    },
    "papermill": {
     "duration": 0.005492,
     "end_time": "2025-04-20T16:53:23.636013",
     "exception": false,
     "start_time": "2025-04-20T16:53:23.630521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TaskFlow: An Autonomous Research & Execution Agent üß†‚öôÔ∏è\n",
    "\n",
    "This project demonstrates a GenAI-based autonomous agent that:\n",
    "- Understands complex research queries\n",
    "- Breaks them into subtasks\n",
    "- Retrieves and summarizes relevant info using LLM\n",
    "- Outputs structured research reports (in JSON/Markdown)\n",
    "\n",
    "Models used: `gemini-1.5-pro`, `text-embedding-004`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d139c1d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:23.649147Z",
     "iopub.status.busy": "2025-04-20T16:53:23.648770Z",
     "iopub.status.idle": "2025-04-20T16:53:25.089659Z",
     "shell.execute_reply": "2025-04-20T16:53:25.088514Z"
    },
    "papermill": {
     "duration": 1.450096,
     "end_time": "2025-04-20T16:53:25.091837",
     "exception": false,
     "start_time": "2025-04-20T16:53:23.641741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4531ff0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:25.105700Z",
     "iopub.status.busy": "2025-04-20T16:53:25.105013Z",
     "iopub.status.idle": "2025-04-20T16:53:25.599947Z",
     "shell.execute_reply": "2025-04-20T16:53:25.598606Z"
    },
    "papermill": {
     "duration": 0.504105,
     "end_time": "2025-04-20T16:53:25.602168",
     "exception": false,
     "start_time": "2025-04-20T16:53:25.098063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_secrets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkaggle_secrets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[0;32m      3\u001b[0m GOOGLE_API_KEY \u001b[38;5;241m=\u001b[39m UserSecretsClient()\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39mGOOGLE_API_KEY)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f332a15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:25.616300Z",
     "iopub.status.busy": "2025-04-20T16:53:25.615876Z",
     "iopub.status.idle": "2025-04-20T16:53:25.882482Z",
     "shell.execute_reply": "2025-04-20T16:53:25.881186Z"
    },
    "papermill": {
     "duration": 0.275907,
     "end_time": "2025-04-20T16:53:25.884661",
     "exception": false,
     "start_time": "2025-04-20T16:53:25.608754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for a complex query, this ensures the client retries if it hits quota limits.\n",
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1f8fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:25.898470Z",
     "iopub.status.busy": "2025-04-20T16:53:25.897727Z",
     "iopub.status.idle": "2025-04-20T16:53:25.991247Z",
     "shell.execute_reply": "2025-04-20T16:53:25.990078Z"
    },
    "papermill": {
     "duration": 0.102443,
     "end_time": "2025-04-20T16:53:25.993254",
     "exception": false,
     "start_time": "2025-04-20T16:53:25.890811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e305d73",
   "metadata": {
    "papermill": {
     "duration": 0.005661,
     "end_time": "2025-04-20T16:53:26.005087",
     "exception": false,
     "start_time": "2025-04-20T16:53:25.999426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Using the gemini-1.5-pro, and answering a prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2645b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:26.018259Z",
     "iopub.status.busy": "2025-04-20T16:53:26.017837Z",
     "iopub.status.idle": "2025-04-20T16:53:30.764381Z",
     "shell.execute_reply": "2025-04-20T16:53:30.762886Z"
    },
    "papermill": {
     "duration": 4.755484,
     "end_time": "2025-04-20T16:53:30.766532",
     "exception": false,
     "start_time": "2025-04-20T16:53:26.011048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several excellent open-source LLMs offer varying capabilities and permissive licenses.\n",
      "\n",
      "**Llama 2** from Meta, under a custom license allowing commercial use but restricting large-scale deployments, provides strong performance across various benchmarks.\n",
      "\n",
      "**Falcon** by TII, licensed under Apache 2.0, offers competitive performance with excellent instruction-following abilities.\n",
      "\n",
      "**MPT** from MosaicML, also Apache 2.0, is designed for commercial usability and efficient inference.\n",
      "\n",
      "**Dolly 2.0** from Databricks, also under Apache 2.0,  focuses on instruction following and is fine-tuned for chat applications.\n",
      "\n",
      "**BLOOM** from BigScience, licensed under RAIL License 1.0, a responsible AI license, is multilingual and emphasizes ethical considerations.\n",
      "\n",
      "Choosing the \"best\" depends on specific needs, considering license implications, performance characteristics, and downstream tasks.  Researching benchmarks and community feedback is crucial for informed selection.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "short_config = types.GenerateContentConfig(max_output_tokens=2000)\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-1.5-pro',\n",
    "    config = short_config,\n",
    "    contents = 'Find best open-source LLMs with license info in 200 words'\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c460d34",
   "metadata": {
    "papermill": {
     "duration": 0.006044,
     "end_time": "2025-04-20T16:53:30.778824",
     "exception": false,
     "start_time": "2025-04-20T16:53:30.772780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Responding some user queries and returning the response in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3bb5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:30.792754Z",
     "iopub.status.busy": "2025-04-20T16:53:30.792377Z",
     "iopub.status.idle": "2025-04-20T16:53:36.184868Z",
     "shell.execute_reply": "2025-04-20T16:53:36.183268Z"
    },
    "papermill": {
     "duration": 5.403069,
     "end_time": "2025-04-20T16:53:36.188120",
     "exception": false,
     "start_time": "2025-04-20T16:53:30.785051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tasks\": [\n",
      "    \"1. Analyze existing AI applications for sustainable development across various sectors (e.g., energy, agriculture, transportation). Identify successful implementations, challenges, and opportunities for improvement.\",\n",
      "    \"2. Investigate the ethical implications of using AI for sustainable development, including bias in algorithms, data privacy, and potential job displacement. Develop guidelines for responsible AI development and deployment.\",\n",
      "    \"3. Evaluate the role of AI in promoting circular economy principles, such as waste reduction, resource optimization, and product lifecycle management. Explore AI-powered solutions for sustainable consumption and production.\",\n",
      "    \"4. Assess the potential of AI to enhance climate change mitigation and adaptation strategies. Analyze how AI can improve climate modeling, prediction, and disaster response.\",\n",
      "    \"5. Develop a framework for integrating AI into national and international sustainable development policies. Identify key stakeholders and recommend strategies for fostering collaboration and innovation.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import typing_extensions as typing\n",
    "import json\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "genai.configure(api_key= GOOGLE_API_KEY)\n",
    "\n",
    "\n",
    "class TaskPlan(typing.TypedDict):\n",
    "    tasks: list[str]\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    temperature=0.2,\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=TaskPlan,\n",
    ")\n",
    "\n",
    "def plan_tasks(topic: str):\n",
    "    response = client.models.generate_content(\n",
    "        model ='gemini-1.5-pro',\n",
    "        config=config,\n",
    "        contents=f\"Break into 3‚Äì5 research tasks: '{topic}'. Return JSON with key 'tasks'.\"\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "user_query = \"Role of AI in sustainable development\" # queries\n",
    "tasks = plan_tasks(user_query)\n",
    "\n",
    "\n",
    "parsed_tasks = json.loads(tasks)\n",
    "print(json.dumps(parsed_tasks, indent=2)) #for proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac487338",
   "metadata": {
    "papermill": {
     "duration": 0.007287,
     "end_time": "2025-04-20T16:53:36.202929",
     "exception": false,
     "start_time": "2025-04-20T16:53:36.195642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Agent Loop\n",
    "[ User query ] ‚Üí [ Task planner ] ‚Üí [ Subtasks ] ‚Üí [ Execute each task ]\n",
    "\n",
    "- It decomposes each tasks into subtasks\n",
    "- Then executes each subtasks using GenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db713ad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:36.216948Z",
     "iopub.status.busy": "2025-04-20T16:53:36.216558Z",
     "iopub.status.idle": "2025-04-20T16:53:42.054376Z",
     "shell.execute_reply": "2025-04-20T16:53:42.052761Z"
    },
    "papermill": {
     "duration": 5.847163,
     "end_time": "2025-04-20T16:53:42.056438",
     "exception": false,
     "start_time": "2025-04-20T16:53:36.209275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Task 1: 1. Analyze existing AI applications for sustainable development across various sectors (e.g., energy, agriculture, transportation). Identify successful implementations, challenges, and opportunities for improvement.\n",
      "üìù Response:\n",
      "AI applications are increasingly contributing to sustainable development across diverse sectors.  Successful implementations include precision agriculture using AI-powered drones for optimized irrigation and fertilization, leading to reduced resource consumption; smart grids leveraging AI for efficient energy distribution and renewable energy integration; and AI-driven traffic management systems optimizing transportation flows and reducing emissions.  However, challenges remain, including data scarcity and bias in training datasets, high computational costs limiting accessibility, and concerns about algorithmic transparency and accountability.  Opportunities for improvement lie in developing more robust, explainable, and ethically sound AI models, fostering greater data sharing and collaboration, and addressing the digital divide to ensure equitable access to these beneficial technologies for sustainable development initiatives globally.\n",
      "\n",
      "\n",
      "üîπ Task 2: 2. Investigate the ethical implications of using AI for sustainable development, including bias in algorithms, data privacy, and potential job displacement. Develop guidelines for responsible AI development and deployment.\n",
      "üìù Response:\n",
      "The application of AI to sustainable development presents a complex ethical landscape.  Algorithmic bias, often reflecting existing societal inequalities embedded in training data, can exacerbate existing environmental injustices, disproportionately impacting vulnerable communities.  Furthermore, the data-intensive nature of AI raises significant privacy concerns, especially when sensitive environmental or demographic information is collected and processed.  Automation driven by AI may lead to job displacement in sectors crucial for sustainable practices, requiring careful consideration of retraining and societal adaptation. Responsible AI development necessitates rigorous audits for bias mitigation, transparent data governance protocols safeguarding privacy, and proactive measures to address potential job losses through reskilling initiatives and equitable distribution of AI's benefits.  Guidelines should prioritize human oversight, explainability of AI decisions, and a focus on societal well-being alongside environmental goals.\n",
      "\n",
      "\n",
      "üîπ Task 3: 3. Evaluate the role of AI in promoting circular economy principles, such as waste reduction, resource optimization, and product lifecycle management. Explore AI-powered solutions for sustainable consumption and production.\n",
      "üìù Response:\n",
      "AI plays a crucial role in advancing circular economy principles by optimizing resource management and minimizing waste across the product lifecycle.  AI-powered predictive maintenance can extend product lifespan, while smart sensors and data analytics can optimize material flows and identify opportunities for reuse and recycling.  In waste management, AI algorithms can improve sorting efficiency and enhance the identification of recyclable materials, leading to higher recovery rates.  Furthermore, AI facilitates design for disassembly and recyclability by simulating product end-of-life scenarios and optimizing material choices.  By enabling more efficient and effective resource allocation and waste reduction strategies, AI-powered solutions are instrumental in transitioning towards sustainable consumption and production patterns.\n",
      "\n",
      "\n",
      "üîπ Task 4: 4. Assess the potential of AI to enhance climate change mitigation and adaptation strategies. Analyze how AI can improve climate modeling, prediction, and disaster response.\n",
      "üìù Response:\n",
      "AI holds significant potential to enhance both climate change mitigation and adaptation strategies.  Its ability to process vast datasets allows for improvements in climate modeling, enabling more accurate predictions of future climate scenarios and the impacts of various mitigation efforts. AI-powered predictive analytics can improve early warning systems for extreme weather events, enhancing disaster response capabilities and minimizing human and economic losses.  Furthermore, AI can optimize energy grids for greater efficiency,  accelerate the development of renewable energy sources, and contribute to the design of more sustainable infrastructure and urban planning.  By automating data analysis and identifying patterns undetectable by humans, AI offers a powerful tool for accelerating our response to the climate crisis.\n",
      "\n",
      "\n",
      "üîπ Task 5: 5. Develop a framework for integrating AI into national and international sustainable development policies. Identify key stakeholders and recommend strategies for fostering collaboration and innovation.\n",
      "üìù Response:\n",
      "Integrating AI into national and international sustainable development policies requires a robust framework emphasizing ethical considerations, data governance, and capacity building.  Key stakeholders include governments, international organizations (e.g., UN, World Bank), private sector AI developers, civil society, and academic researchers.  Fostering collaboration necessitates establishing multi-stakeholder platforms for dialogue and knowledge sharing, promoting open-source AI tools and datasets for equitable access, and investing in education and training programs to build AI literacy.  Strategies should prioritize inclusive development, ensuring AI benefits all segments of society, while mitigating potential risks like bias and job displacement.  Incentivizing collaborative research projects focused on specific sustainable development goals (SDGs), such as climate change mitigation and resource management, will drive innovation and effective policy implementation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\") # using a different model as gemini 1.5 pro has limitations on rpm\n",
    "\n",
    "\n",
    "\n",
    "for i, task in enumerate(parsed_tasks['tasks'], 1):\n",
    "    print(f\"\\nüîπ Task {i}: {task}\")\n",
    "    \n",
    "    # Ask Gemini to answer each task\n",
    "    response = model.generate_content(f\"Write a short paragraph on: {task}\")\n",
    "    \n",
    "    print(\"üìù Response:\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e7fe1",
   "metadata": {
    "papermill": {
     "duration": 0.006485,
     "end_time": "2025-04-20T16:53:42.070069",
     "exception": false,
     "start_time": "2025-04-20T16:53:42.063584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Document Understanding\n",
    "\n",
    "Reading documents and extracting text from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad31437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:42.086986Z",
     "iopub.status.busy": "2025-04-20T16:53:42.086481Z",
     "iopub.status.idle": "2025-04-20T16:53:49.745776Z",
     "shell.execute_reply": "2025-04-20T16:53:49.744428Z"
    },
    "papermill": {
     "duration": 7.671201,
     "end_time": "2025-04-20T16:53:49.747934",
     "exception": false,
     "start_time": "2025-04-20T16:53:42.076733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\r\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\r\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pymupdf\r\n",
      "Successfully installed pymupdf-1.25.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "raw_text = extract_text_from_pdf('/kaggle/input/syllabus/btech_cse_3rd_year_1624712911.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb36799",
   "metadata": {
    "papermill": {
     "duration": 0.008487,
     "end_time": "2025-04-20T16:53:49.765152",
     "exception": false,
     "start_time": "2025-04-20T16:53:49.756665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Breaking the long pdf into chunks so that it is made easier for the model to understand and give output efficiently with minimal errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2bfab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:49.782139Z",
     "iopub.status.busy": "2025-04-20T16:53:49.781771Z",
     "iopub.status.idle": "2025-04-20T16:53:49.790303Z",
     "shell.execute_reply": "2025-04-20T16:53:49.788686Z"
    },
    "papermill": {
     "duration": 0.019776,
     "end_time": "2025-04-20T16:53:49.792437",
     "exception": false,
     "start_time": "2025-04-20T16:53:49.772661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=100, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(raw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76e909",
   "metadata": {
    "papermill": {
     "duration": 0.006967,
     "end_time": "2025-04-20T16:53:49.807647",
     "exception": false,
     "start_time": "2025-04-20T16:53:49.800680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Embed the chunks\n",
    "\n",
    "Embedding the chunks into vectors for semantic understanding.\n",
    "\n",
    "using embedding model for embedding the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5206b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:53:49.823690Z",
     "iopub.status.busy": "2025-04-20T16:53:49.823266Z",
     "iopub.status.idle": "2025-04-20T16:54:12.838397Z",
     "shell.execute_reply": "2025-04-20T16:54:12.837066Z"
    },
    "papermill": {
     "duration": 23.025452,
     "end_time": "2025-04-20T16:54:12.840309",
     "exception": false,
     "start_time": "2025-04-20T16:53:49.814857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batching the chunks as the request is limited to 100\n",
    "def batch_chunks(chunks, batch_size=100):\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        yield chunks[i:i + batch_size]\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for batch in batch_chunks(chunks, batch_size=100):\n",
    "    response = client.models.embed_content(\n",
    "        model='models/text-embedding-004',\n",
    "        contents=batch,\n",
    "        config=types.EmbedContentConfig(task_type='semantic_similarity')\n",
    "    )\n",
    "    all_embeddings.extend([r.values for r in response.embeddings])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea91648",
   "metadata": {
    "papermill": {
     "duration": 0.007897,
     "end_time": "2025-04-20T16:54:12.855902",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.848005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The embeddedd vectors are now being stored in chunk_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648020f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:12.871914Z",
     "iopub.status.busy": "2025-04-20T16:54:12.871560Z",
     "iopub.status.idle": "2025-04-20T16:54:12.888671Z",
     "shell.execute_reply": "2025-04-20T16:54:12.887262Z"
    },
    "papermill": {
     "duration": 0.027561,
     "end_time": "2025-04-20T16:54:12.890922",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.863361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming all_embeddings and chunks are in order\n",
    "chunk_db = [\n",
    "    {\"text\": chunk, \"embedding\": embedding}\n",
    "    for chunk, embedding in zip(chunks, all_embeddings)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca2831",
   "metadata": {
    "papermill": {
     "duration": 0.008326,
     "end_time": "2025-04-20T16:54:12.907874",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.899548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Also embedding the query into vectors so that the model can match these embedding with the relevant content from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78860ca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:12.924378Z",
     "iopub.status.busy": "2025-04-20T16:54:12.923962Z",
     "iopub.status.idle": "2025-04-20T16:54:12.928907Z",
     "shell.execute_reply": "2025-04-20T16:54:12.927409Z"
    },
    "papermill": {
     "duration": 0.01543,
     "end_time": "2025-04-20T16:54:12.930839",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.915409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"what is the topics of soft computing and explain each topic within it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b62bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:12.947075Z",
     "iopub.status.busy": "2025-04-20T16:54:12.946742Z",
     "iopub.status.idle": "2025-04-20T16:54:13.127730Z",
     "shell.execute_reply": "2025-04-20T16:54:13.126568Z"
    },
    "papermill": {
     "duration": 0.19152,
     "end_time": "2025-04-20T16:54:13.130032",
     "exception": false,
     "start_time": "2025-04-20T16:54:12.938512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_embedding_response = client.models.embed_content(\n",
    "    model='models/text-embedding-004',\n",
    "    contents=[query],\n",
    "    config=types.EmbedContentConfig(task_type='semantic_similarity')\n",
    ")\n",
    "query_embedding = query_embedding_response.embeddings[0].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe62053",
   "metadata": {
    "papermill": {
     "duration": 0.006919,
     "end_time": "2025-04-20T16:54:13.144249",
     "exception": false,
     "start_time": "2025-04-20T16:54:13.137330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Calculating the similarity scores using cosine similarity and finding the top k similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092ac5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:13.160454Z",
     "iopub.status.busy": "2025-04-20T16:54:13.160018Z",
     "iopub.status.idle": "2025-04-20T16:54:13.536617Z",
     "shell.execute_reply": "2025-04-20T16:54:13.535385Z"
    },
    "papermill": {
     "duration": 0.388596,
     "end_time": "2025-04-20T16:54:13.540117",
     "exception": false,
     "start_time": "2025-04-20T16:54:13.151521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "# Find top-k most relevant chunks\n",
    "top_k = 3\n",
    "scored_chunks = [\n",
    "    (cosine_similarity(np.array(query_embedding), np.array(chunk['embedding'])), chunk['text'])\n",
    "    for chunk in chunk_db\n",
    "]\n",
    "\n",
    "# Sort by score\n",
    "ranked_chunks = sorted(scored_chunks, key=lambda x: x[0], reverse=True)\n",
    "top_chunks = [text for _, text in ranked_chunks[:top_k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a5a59",
   "metadata": {
    "papermill": {
     "duration": 0.007553,
     "end_time": "2025-04-20T16:54:13.557185",
     "exception": false,
     "start_time": "2025-04-20T16:54:13.549632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Giving a rag prompt that specifies what to do to the model\n",
    "\n",
    "and generating the output using the gemini model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d43e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:54:13.575184Z",
     "iopub.status.busy": "2025-04-20T16:54:13.574705Z",
     "iopub.status.idle": "2025-04-20T16:54:19.655672Z",
     "shell.execute_reply": "2025-04-20T16:54:19.653583Z"
    },
    "papermill": {
     "duration": 6.092854,
     "end_time": "2025-04-20T16:54:19.657782",
     "exception": false,
     "start_time": "2025-04-20T16:54:13.564928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final Answer:\n",
      " The context mentions three main topics within soft computing:\n",
      "\n",
      "1. **Fuzzy Logic:**  Fuzzy logic deals with uncertainty and imprecise information. Unlike traditional binary logic (true/false), fuzzy logic allows for degrees of truth.  It uses fuzzy sets, which can have partial membership. For example, the concept of \"tall\" is fuzzy. Someone who is 6'2\" is definitely tall, someone who is 5'2\" is definitely not tall, but someone who is 5'10\" might have a partial membership in the set of \"tall\" people. This allows for more nuanced and human-like reasoning in situations where precise definitions are difficult or impossible.\n",
      "\n",
      "2. **Neural Networks:** Inspired by the biological nervous system, neural networks are computing systems made of interconnected nodes (neurons) organized in layers.  These networks learn from data by adjusting the strengths of the connections between neurons. They are particularly good at pattern recognition, classification, and prediction tasks, and are used in areas like image recognition, natural language processing, and robotics.\n",
      "\n",
      "3. **Evolutionary Computing:** This field draws inspiration from biological evolution. It uses algorithms that mimic processes like natural selection, mutation, and crossover to find optimal solutions to complex problems. Genetic algorithms are a common example, where potential solutions are represented as \"chromosomes\" and evolved over generations to find the fittest solution.  Evolutionary computing is often used for optimization, search, and machine learning tasks. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_prompt = (\n",
    "    \"Answer the following question using the provided context.\\n\\n\"\n",
    "    f\"Context:\\n{''.join(top_chunks)}\\n\\n\"\n",
    "    f\"Question:\\n{query}\"\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "response = model.generate_content(rag_prompt)\n",
    "\n",
    "print(\"üîç Final Answer:\\n\", response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7109811,
     "sourceId": 11359831,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.316495,
   "end_time": "2025-04-20T16:54:22.914277",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T16:53:08.597782",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
